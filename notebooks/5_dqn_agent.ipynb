{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5099098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials: \n",
    "# https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb#scrollTo=N7brXNIGWXjC\n",
    "# https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb?hl=da-dk#scrollTo=1ZAoFNwnRbKK\n",
    "# https://medium.com/deep-learning-journals/train-your-dog-using-tf-agents-fba297a85baa\n",
    "\n",
    "# tensorflow tf agents series:\n",
    "# https://www.tensorflow.org/agents/tutorials/2_environments_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdeafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import py_environment, tf_py_environment, utils\n",
    "from tf_agents.trajectories import Trajectory, time_step as ts\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.policies.q_policy import QPolicy\n",
    "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
    "\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd298857",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "\n",
    "# Env settings\n",
    "num_actions = (100-20) / 2   # (40 price points by steps of 2)\n",
    "input_features = 5  # TODO: Make dynamic\n",
    "\n",
    "# selling_period\n",
    "# price_competitor\n",
    "# price\n",
    "# demand\n",
    "# competitor_has_capacity\n",
    "\n",
    "# Replay buffer settings\n",
    "batch_size = 1\n",
    "max_length = 1000\n",
    "\n",
    "# Neural net settings\n",
    "learning_rate = 1e-3\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "\n",
    "# Set seed for reproducability\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5de8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment that the agent can interact with, and we can alter remotely\n",
    "class DynamicPricingCompetition():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.selling_period = 1\n",
    "        self.price = 50\n",
    "        self.competitor_price = 50\n",
    "        self.demand = 0\n",
    "        self.competitor_has_capacity = 1\n",
    "        self.state = [\n",
    "            self.selling_period, \n",
    "            self.price, \n",
    "            self.competitor_price, \n",
    "            self.demand, \n",
    "            self.competitor_has_capacity\n",
    "        ]\n",
    "        self._reward = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.selling_period = 1\n",
    "        self.price = 50\n",
    "        self.competitor_price = 50\n",
    "        self.demand = 0\n",
    "        self.competitor_has_capacity = 1\n",
    "        self.state = [\n",
    "            self.selling_period, \n",
    "            self.price, \n",
    "            self.competitor_price, \n",
    "            self.demand, \n",
    "            self.competitor_has_capacity\n",
    "        ]\n",
    "        self._reward = 0\n",
    "        \n",
    "    def update_state(self, selling_period, price, competitor_price, demand, competitor_has_capacity):\n",
    "        self.selling_period = selling_period\n",
    "        self.price = price\n",
    "        self.competitor_price = competitor_price\n",
    "        self.demand = demand\n",
    "        self.competitor_has_capacity = competitor_has_capacity\n",
    "        self.state = [\n",
    "            self.selling_period, \n",
    "            self.price, \n",
    "            self.competitor_price, \n",
    "            self.demand, \n",
    "            self.competitor_has_capacity\n",
    "        ]\n",
    "        \n",
    "    def update_reward(self, reward):\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad9bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment in which the agent operates in, and is protected from altering\n",
    "\n",
    "class AirlineEnvironment(py_environment.PyEnvironment):\n",
    "    \n",
    "    def __init__(self, dpc_game, discount=1.0):\n",
    "        \"\"\"\n",
    "        Initialize what actions the agent can take,\n",
    "        and what the observation space will look like.\n",
    "        \n",
    "        Also initialize the environment where the agent will interact with.\n",
    "        \"\"\"\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=num_actions-1, name='action'\n",
    "        )\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(input_features,), dtype=np.int32, name='observation',\n",
    "            minimum=np.array([1., 0., 0., 0., 0.], dtype=np.float32), \n",
    "            maximum=np.array([100., 1000., 1000., 80., 1.], dtype=np.float32)\n",
    "        )\n",
    "        self._episode_ended = False\n",
    "        self._discount = discount\n",
    "        self._dpc_game = dpc_game\n",
    "        \n",
    "        # TODO: Map discrete actions here to price points\n",
    "        \n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "    \n",
    "    def current_time_step(self):\n",
    "        return self._current_time_step\n",
    "\n",
    "    def reset(self):\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "\n",
    "    def _reset(self):\n",
    "        self._episode_ended = False\n",
    "        self._dpc_game.reset()\n",
    "        return ts.restart(np.array(self._dpc_game.state, dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        \n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        # Make sure episodes don't go on forever.\n",
    "        if self._dpc_game.state[0] == 100:\n",
    "            self._episode_ended = True\n",
    "            return ts.termination(\n",
    "                np.array(self._dpc_game.state, dtype=np.int32), \n",
    "                self._dpc_game.reward\n",
    "            )\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array(self._dpc_game.state, dtype=np.int32), \n",
    "                reward=self._dpc_game.reward, \n",
    "                discount=self._discount\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba375d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate environment\n",
    "dpc_game = DynamicPricingCompetition()\n",
    "environment = AirlineEnvironment(dpc_game)\n",
    "\n",
    "# TODO: Will need a class that creates random input when called to validate env\n",
    "#utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "572feb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and evaluate env\n",
    "train_env = tf_py_environment.TFPyEnvironment(environment)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91ef9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': BoundedTensorSpec(shape=(5,), dtype=tf.int32, name='observation', minimum=array([1, 0, 0, 0, 0]), maximum=array([ 100, 1000, 1000,   80,    1])),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the specs of one time step\n",
    "train_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b77e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network for agent\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "layer1 = Dense(units=50, input_shape=(input_features,), activation='relu', name='hidden_layer1')\n",
    "layer2 = Dense(units=100, activation='relu', name='hidden_layer2')\n",
    "layer3 = Dense(units=num_actions, activation=None)\n",
    "q_net = sequential.Sequential([layer1, layer2, layer3])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2813b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent itself\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    time_step_spec=train_env.time_step_spec(),\n",
    "    action_spec=train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    n_step_update=1,\n",
    "#     td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter\n",
    ")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd86f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy: \n",
    "# A Q policy is used in agents like DQN and is based on a Q network that predicts a Q value for each discrete action. \n",
    "# For a given time step, the action distribution in the Q Policy is a categorical distribution created using \n",
    "# the q values as logits.\n",
    "q_policy = QPolicy(train_env.time_step_spec(), train_env.action_spec(), q_network=q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba18de98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 1, 50, 50,  0,  1]])>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First step for agent (clean env)\n",
    "time_step = train_env.reset()\n",
    "time_step\n",
    "# Step type is 0, reward is 0, observation is based on initialization state, discount adjustted at env creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65bf23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First action\n",
    "action_step = q_policy.action(time_step, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f20061b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([24])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_step.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f17856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with env here and get new env vars, this will happen at every p() call\n",
    "selling_period = 2\n",
    "price = 20 + 2*int(action_step.action)\n",
    "competitor_price = 48\n",
    "demand = 1\n",
    "competitor_has_capacity = 1\n",
    "\n",
    "reward = demand * price\n",
    "\n",
    "dpc_game.update_state(selling_period, price, competitor_price, demand, competitor_has_capacity)\n",
    "dpc_game.update_reward(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5e69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = train_env.step(action_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e077efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 2, 68, 48,  1,  1]])>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([68.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c864f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second action\n",
    "action_step = q_policy.action(time_step, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bfc663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([37])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_step.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d00f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[100,  77,  80,   1,   0]])>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([77.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])>})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if episode ends right\n",
    "\n",
    "selling_period = 100\n",
    "price = 77\n",
    "competitor_price = 80\n",
    "demand = 1\n",
    "competitor_has_capacity = 0\n",
    "\n",
    "reward = demand * price\n",
    "\n",
    "dpc_game.update_state(selling_period, price, competitor_price, demand, competitor_has_capacity)\n",
    "dpc_game.update_reward(reward)\n",
    "\n",
    "time_step = train_env.step(action_step.action)\n",
    "time_step\n",
    "# Step type is now 2 (end), so that correctly ends the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b2e8263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 1, 50, 50,  0,  1]])>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first next step is disregarded (i.e. predicting for the next step in a terminated episode)\n",
    "# and resets the environment to a new episode\n",
    "\n",
    "action_step = q_policy.action(time_step, seed=seed)\n",
    "action_step\n",
    "\n",
    "selling_period = 1\n",
    "price = 60\n",
    "competitor_price = 40\n",
    "demand = 0\n",
    "competitor_has_capacity = 1\n",
    "\n",
    "reward = demand * price\n",
    "\n",
    "dpc_game.update_state(selling_period, price, competitor_price, demand, competitor_has_capacity)\n",
    "dpc_game.update_reward(reward)\n",
    "\n",
    "time_step = train_env.step(action_step.action)\n",
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed956c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer (training)\n",
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ee8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate step-action-step\n",
    "time_step = train_env.reset()\n",
    "action_step = q_policy.action(time_step, seed=seed)\n",
    "dpc_game.update_state(selling_period, price, competitor_price, demand, competitor_has_capacity)\n",
    "dpc_game.update_reward(reward)\n",
    "next_time_step = train_env.step(action_step.action)\n",
    "\n",
    "# Package information into a trajectory\n",
    "traj = Trajectory(\n",
    "    time_step.step_type,\n",
    "    time_step.observation,\n",
    "    action_step.action,\n",
    "    action_step.info,\n",
    "    next_time_step.step_type,\n",
    "    next_time_step.reward,\n",
    "    next_time_step.discount\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16243747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([24])>,\n",
       " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>,\n",
       " 'observation': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 1, 50, 50,  0,  1]])>,\n",
       " 'policy_info': (),\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21443c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using batch update\n",
    "# batch = tf.nest.map_structure(lambda t: tf.expand_dims(t, 0), traj)\n",
    "# replay_buffer.add_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73bec068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add trajectory to the replay buffer\n",
    "for _ in range(6):\n",
    "    replay_buffer.add_batch(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf318d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=6>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.num_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b3b7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training using the replay buffer\n",
    "dataset = replay_buffer.as_dataset(sample_batch_size=4, num_steps=2, single_deterministic_pass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "455f3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\u02vpe\\Anaconda3\\envs\\dynamic_pricing_comp\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "trajectories, _ = next(iterator)\n",
    "loss = agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cdf4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "for i in range(100):\n",
    "    trajectories, _ = next(iterator)\n",
    "    loss = agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b43c99d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': <tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "array([[24, 24],\n",
       "       [24, 24],\n",
       "       [24, 24],\n",
       "       [24, 24]])>,\n",
       " 'discount': <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]], dtype=float32)>,\n",
       " 'next_step_type': <tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])>,\n",
       " 'observation': <tf.Tensor: shape=(4, 2, 5), dtype=int32, numpy=\n",
       "array([[[ 1, 50, 50,  0,  1],\n",
       "        [ 1, 50, 50,  0,  1]],\n",
       "\n",
       "       [[ 1, 50, 50,  0,  1],\n",
       "        [ 1, 50, 50,  0,  1]],\n",
       "\n",
       "       [[ 1, 50, 50,  0,  1],\n",
       "        [ 1, 50, 50,  0,  1]],\n",
       "\n",
       "       [[ 1, 50, 50,  0,  1],\n",
       "        [ 1, 50, 50,  0,  1]]])>,\n",
       " 'policy_info': (),\n",
       " 'reward': <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]])>})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023809e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic_pricing_comp",
   "language": "python",
   "name": "dynamic_pricing_comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
